---
layout: article
title: Linux内核中的网络负载均衡：ECMP
mathjax: true
mermaid: true
chart: true
mathjax_autoNumber: true
toc: true
mode: immersive
tags: 网络 Linux
key: nat6
header:
  theme: dark
article_header:
  type: overlay
  theme: dark
  background_color: '#ffffff'
  background_image:
    src: https://i.loli.net/2019/11/04/TmsPRIL53X9yq7p.png
    gradient: 'linear-gradient(0deg, rgba(0, 0, 0 , .7), rgba(0, 0, 0, .7))'
---

因为偶然发现了ECMP这种负载均衡的方法，然后查了些资料，这里整理下；文末留下了当时发现ECMP的记录

<!--more-->
## 参考

ECMP在工程方面用的很多，能找到的多是说明文档，介绍特性和成套的解决方案，但是基础的材料并不是那么好找，个人也是刚刚接触到ECMP，水平有限，这里先给出本文的主要参考文献

系列博文：深入浅出，推荐通读
- [Set Up & History dive](https://codecave.cc/multipath-routing-in-linux-part-1.html)
- [Two stacks, two stories](https://codecave.cc/multipath-routing-in-linux-part-2.html)
- [Problems & Recent Developments](https://codecave.cc/multipath-routing-ecmp-in-linux-part-3.html)

相对详尽的中文资料，重点是ECMP在内核中的变更历史：[ECMP在Linux内核的实现](https://cloud.tencent.com/developer/article/1449969)

华为的文档：主要针对load-balance做了介绍，建议还是看英文的吧，翻译一言难尽：[负载分担介绍](https://support.huawei.com/enterprise/zh/doc/EDOC1100055151/923df53b)

## Linux内核中ECMP

对参考中的材料做的总结，这里还有与时间对应的OpenWrt的内核版本信息：

| Linux Kernel    | OpenWrt 内核版本   | ECMP变更                      | ECMP形式                                     |
| --------------- | ----------------- | ----------------------------- | -------------------------------------------- |
| 1997 \| 2.1.68  |                   | IPV4 ECMP 加入内核            | L3 Per-packet + route cache -> L3 Per-flow   |
| 2007 \| 2.6.23  |                   | IPV4 multipath cached 移除    | L3 Per-packet + route cache -> L3 Per-flow   |
| 2012.9 \| 3.6   |                   | IPV4 route cache 被移除       | L3 Per-packet -> L3 Per-packet               |
| 2012.10 \| 3.8  |                   | IPV6 ECMP 加入内核            | IPv6 Flowlabel -> L4 Per-flow                |
| 2015.9 \| 4.4   | 15.05 \| 3.18     | IPV4 multipath 特性重新被加回 | L3 Per-packet + L3 hash -> L3 Per-flow       |
| 2016.4 \| 4.7   |                   | IPV4 ECMP 增加邻居健康检查    |                                              |
| 2017.2          | 17.01 \| 4.4      |                               |                                              |
| 2017.3 \| 4.12  |                   | IPV4 ECMP 增加 L4 Hash        | L3 Per-packet + L3/L4 hash -> L3/L4 Per-flow |
| 2017.11 \| 4.14 |                   | IPV6 ECMP: ICMPv6 修复        |                                              |
| 2018.4 \| 4.16  |                   | IPv6 ECMP: 支持指定权重       |                                              |
| 2018.7          | 18.06 \| 4.9/4.14 |                               |                                              |
| 2018.10 \| 4.19 |                   |                               |                                              |

## 一些概念

### Flow
对路由器而言，流是共享某些特性的packet序列，比如同一个请求（连接）的packets有相同的路由路径，

### IPv6 Flowlabel
IPv6数据报中有一个20字节的字段：流标号（流标签）；进而可以用流标号取代路由表来处理流的路由，加速路由器对分组的处理

## 启用ECMP

### 添加ECMP默认路由

这里直接引用iproute2的user Guide：

[→ ](https://baturin.org/docs/iproute2/#Multipath routing)Multipath routing

```
ip route add ${addresss}/${mask} nexthop via ${gateway 1} weight ${number} nexthop via ${gateway 2} weight ${number}
```

Multipath routes make the system balance packets across several links according to the weight (higher weight is preferred, so gateway/interface with weight of 2 will get roughly two times more traffic than another one with weight of 1). You can have as many gateways as you want and mix gateway and interface routes, like:

```
ip route add default nexthop via 192.168.1.1 weight 1 nexthop dev ppp0 weight 10
```

**Warning:** the downside of this type of balancing is that packets are not guaranteed to be sent back through the same link they came in. This is called "asymmetric routing". For routers that simply forward packets and don't do any local traffic processing such as NAT, this is usually normal, and in some cases even unavoidable.

If your system does anything but forwarding packets between interfaces, this may cause problems with incoming connections and some measures should be taken to prevent it.

可以补充的是如果用的多个等带宽的PPPoE Interface的话，可以只用```nexthop dev ${pppoe-dev}```

### 修改内核运行参数

仅仅是上一步的话，可以见到某些情况下已经是有负载均衡了，但是未达到mwan3负载均衡的那种效果，因为OpenWrt对IPv4的ECMP默认设置到L3 Hash，而且也没有开启邻居健康检查，所以需要修改下内核的运行参数，相关的具体的参数说明如下：

> fib_multipath_hash_policy - INTEGER
>	Controls which hash policy to use for multipath routes. Only valid for kernels built with CONFIG_IP_ROUTE_MULTIPATH enabled.
>	Default: 0 (Layer 3)
>	Possible values:
>	0 - Layer 3
>	1 - Layer 4
>	2 - Layer 3 or inner Layer 3 if present

> fib_multipath_use_neigh - BOOLEAN
>	Use status of existing neighbor entry when determining nexthop for multipath routes. If disabled, neighbor information is not used and packets could be directed to a failed nexthop. Only valid for kernels built with CONFIG_IP_ROUTE_MULTIPATH enabled.
>	Default: 0 (disabled)
>	Possible values:
>	0 - disabled
>	1 - enabled

```shell
echo "net.ipv4.fib_multipath_hash_policy=1" >> /etc/sysctl.conf
echo "net.ipv4.fib_multipath_use_neigh=1" >> /etc/sysctl.conf
sysctl -p
```


## 偶然的发现

这里保留当时的发现过程：（当时发现了这个功能非常开心，现在来看部分内容不准确）

只在K2P OpenWrt 18.06上面实践过，对LEDE 17.01无效

下面是在操作一番之后的路由表，对IPv6的测速显示网速翻了四倍

```shell
root@OpenWrt:~# ip -6 route | grep pppoe
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN4 proto static metric 512 pref medium
2000:::/3 dev pppoe-VWAN4 proto static metric 256 pref medium
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-wan weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN4 weight 1
```

大致可以看出这已经是做了负载均衡，需要的只是安装好luci-app-mwan3，简单操作一下路由表，这里的网关地址和dev需要看具体情况

2000::/3 就是IPv6的单播（Unicast）地址了，在不做任何操纵的情况下，无PD的路由表（单拨）

```shell
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 proto static metric 512 pref medium
2001:250:1006:dff0::/64 dev pppoe-VWAN3 proto static metric 256 pref medium
...
```
之后通过下面两条常规的添加路由表条目的命令：
```shell
route -A inet6 add 2000::/3 gw fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2
route -A inet6 add 2000::/3 gw fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3
```
发现路由表就出现了nexthup和负载均衡中的weight标记

```shell
root@OpenWrt:~# ip -6 route | grep pppoe
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 proto static metric 512 pref medium
2000::/3 dev pppoe-VWAN3 proto static metric 256 pref medium
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 weight 1
...
```

类似的网关添加过程对IPv4效果并不好，这里稍后再细说

之后还需要修改一下防火墙，这里用的还是NAT6中的那条命令

因为对路由表的修改在路由器重启之后会重置，所以还是要添加到开机的启动脚本或者添加到自定的防火墙规则之中，如果已经在NAT6配置过程中配置好了的话这一步就可以忽略了

```shell
ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE
```

实测的峰值速度可以到达双网口的满速，但是一样的问题，在UT，IDM，Thunder这种多线程下载软件下轻松满速，而在YouTube视频应用下，只有半速（甚至包括用IDM下载的情况下）

目前最新版本的mwan3也开始支持IPv6多拨了，但是个人还是觉得日常使用的话用以上的方法更加快捷，不过需要对网络有一定的了解，下面是一段针对IPv6 NAT的多拨hotplug脚本，用了一些OpenWrt开发时推荐的写法，比较实验性

```shell
#!/bin/sh
[ "$ACTION" = ifup ] || exit 0
ifaces=$(ubus call network.interface dump | jsonfilter -e '$.interface[@.proto="dhcpv6" && @.up=true].interface')
for iface_6 in $ifaces
do
  [ "$INTERFACE" = $iface_6 ]  || continue
  devices=$(ubus call network.interface dump | jsonfilter -e '$.interface[@.proto="dhcpv6" && @.up=true].device')
  ipv6_gw=$(ifstatus $iface_6 | jsonfilter -e '$.route[1].nexthop')
  for ipv6_dev in $devices
  do
    status=$(route -A inet6 add 2000::/3 gw $ipv6_gw dev $ipv6_dev 2>&1)
    logger -t NAT6 "Gateway: $ipv6_dev: Done $status"
  done
  exit 0
done
```
