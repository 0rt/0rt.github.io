---
layout: article
title: 网络负载均衡:ECMP
mathjax: true
mermaid: true
chart: true
mathjax_autoNumber: true
toc: true
mode: immersive
tags: Linux 网络
key: ECMP
header:
  theme: dark
article_header:
  type: overlay
  theme: dark
  background_color: '#ffffff'
  background_image:
    src: https://i.loli.net/2019/11/04/TmsPRIL53X9yq7p.png
    gradient: 'linear-gradient(0deg, rgba(0, 0, 0 , .7), rgba(0, 0, 0, .7))'
---

因为偶然发现了Linux内核中的ECMP这种负载均衡的方法，然后查了些资料，这里整理下(业余水平)；文末留下了当时发现ECMP的记录

<!--more-->
## 参考

ECMP在工程方面用的很多，能找到的多是说明文档，介绍特性和成套的解决方案，但是基础的材料并不是那么好找，个人也是刚刚接触到ECMP，水平有限，这里先给出本文的主要参考文献

[Jakub Sitnicki's Blog](https://codecave.cc/author/jakub-sitnicki.html)的系列博文：深入浅出，推荐通读
- [Set Up & History dive](https://codecave.cc/multipath-routing-in-linux-part-1.html)
- [Two stacks, two stories](https://codecave.cc/multipath-routing-in-linux-part-2.html)
- [Problems & Recent Developments](https://codecave.cc/multipath-routing-ecmp-in-linux-part-3.html)

相对详尽的中文资料，重点是ECMP在内核中的变更历史：[ECMP在Linux内核的实现](https://cloud.tencent.com/developer/article/1449969)

华为的文档：主要针对load-balance的概念做了介绍，建议还是看英文的吧，翻译一言难尽：[负载分担介绍](https://support.huawei.com/enterprise/zh/doc/EDOC1100055151/923df53b)

## Linux内核中ECMP

Linux内核的Git [Commit](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/)

这里把ECMP的变更历史整理到一张表格中，还有与时间对应的OpenWrt的内核版本信息：

| **Linux Kernel**|**OpenWrt 内核版本**| **ECMP变更**                  | **ECMP形式**                            |
| --------------- | ----------------- | ----------------------------- | -------------------------------------------- |
| 1997 \| 2.1.68  |                   | IPV4 ECMP 加入内核             | L3 Per-packet + route cache -> L3 Per-flow   |
| 2007 \| 2.6.23  |                   | IPV4 multipath cached 移除    | L3 Per-packet + route cache -> L3 Per-flow   |
| 2012.9 \| 3.6   |                   | IPV4 route cache 被移除       | L3 Per-packet -> L3 Per-packet               |
| 2012.10 \| 3.8  |                   | IPV6 ECMP 加入内核            | IPv6 Flowlabel -> L4 Per-flow                |
| 2015.9 \| 4.4   | 15.05 \| 3.18     | IPV4 ECMP：使用L3 Hash        | L3 Per-packet + L3 hash -> L3 Per-flow       |
| 2016.4 \| 4.7   |                   | IPV4 ECMP: 增加邻居健康检查    |                                              |
| 2017.2          | 17.01 \| 4.4      |                               |                                              |
| **2017.3 \| 4.12**|                 |**IPV4 ECMP: 增加 L4 Hash**    | L3 Per-packet + L3/L4 hash -> L3/L4 Per-flow |
| 2017.11 \| 4.14 |                   | IPV6 ECMP: ICMPv6 修复        |                                              |
| 2018.4 \| 4.16  |                   | IPv6 ECMP: 支持指定权重        |                                              |
| 2018.7        |**18.06 \| 4.9/4.14**|                               |                                              |
| 2018.10 \| 4.19 |                   |                               |                                              |

这里可以推出（实际早期版本我也没有测试过），在默认的编译选项和内核版本下，较为实用的L4负载均衡，IPv4需要在18.06及之后的版本

IPv6由于使用了flowlabel作为Hash的对象，应该一开始就可以L4负载均衡，看表格的话，OpenWrt在15.05之后的版本就可以实现

OpenWrt中的具体实现可以看博客中的[Speed_Up](https://lwz322.github.io/2018/10/08/Speed-Up.html#ecmp)

## 一些概念

基础的概念可以看前面提到的华为的[文档](https://support.huawei.com/enterprise/my/doc/EDOC1100058404/102b770f/load-balancing)，下面是一些关键概念

### Packet/Flow
Packet对应的是IP分组(数据报、数据包)，是数据在L3上传输的单元

对路由器而言，流(Flow)是共享某些特性的packet序列，比如同一个请求（连接）的packets有相同的路由路径

在负载均衡中有Per-Packet和Per-Flow两种方式，[文档](https://support.huawei.com/enterprise/my/doc/EDOC1100058404/ebc8ad42/per-flow-and-per-packet-load-balancing)中的两张图很好的对比了这两种方式：
![](https://i.loli.net/2019/11/04/EztsLnah3GYm7eB.png)

显然分包负载均衡的效率更高，而Linux内核实现的ECMP依然是分流(Per-Flow)负载均衡，和之前的OpenWrt的mwan3的效果的差异不大(效率可能高一点)，可以扩展到更多的平台

> It is important to keep in mind that existing Multipath Routing implementation in Linux is designed to **distribute flows of packets over multiple paths**, not individual packets. Selecting route in a per-packet manner does not play well with TCP, IP fragments, or Path MTU Discovery.

### IPv6 Flowlabel
IPv6数据报(packet)中有一个20字节的字段：流标号（流标签）；进而可以用流标号取代路由表来处理流的路由，加速路由器对分组的处理；在ECMP中却提供了Flow的信息，故IPv6 ECMP是比较容易做的

### L3/L4
在网络模型中，L3也就是网络层，平时用的多是IP协议，L4是传输层，如常用的TCP/UDP协议，而ECMP作用的层级对效果是至关重要的：通过Packet的首部信息把一个个的packet和Flow联系起来，进而进行下一跳的路由选择

> To associate a packet with a flow, the net stack computes a hash over a **subset of packet header fields**. The resulting hash value is what drives the next-hop selection. In Linux v4.11 the selection of fields depends on the IP protocol version and whether we are forwarding or routing locally generated packets. The fields that serve as input to hashing function are:

- for forwarded IPv4 packets (L3 hash)
  ```
  { Source Address, Destination Address }
  ```
  使用路由表就可以做到这一步

- for locally generated IPv4 packets (L4 hash) 
  ```
  { Source Address, Destination Address, Protocol, Source Port, Destination Port }
  ```
  结合策略路由和iptables mark可以做到类似的效果

- for forwarded IPv6 packets (L3 hash)
  ```
  { Source Address, Destination Address, Flow Label, Next Header (protocol) }
  ```
  因为有Flow Label的存在，IPv6即使只用到L3 Hash也可以实现L4-Flow负载均衡

- for locally generated IPv6 packets (L4 hash)
  ```
  { Source Address, Destination Address, Flow Label, Next Header (protocol), Source Port, Destination Port }
  ```

## 启用ECMP

准备部分是从排查问题的角度来看的

在启用之前可以确认下编译内核时的下面选项，较新版本的OpenWrt默认是打开的

- CONFIG_IP_ROUTE_MULTIPATH=y
  IP: equal cost multipath，ECMP支持

如果需要使用命令行工具的话，可能需要iproute2（OpenWrt默认是ip-tiny，也可以用）

### 添加ECMP路由

这里直接引用iproute2的user Guide：

[→ ](https://baturin.org/docs/iproute2/#Multipath routing)Multipath routing

```
ip route add ${addresss}/${mask} nexthop via ${gateway 1} weight ${number} nexthop via ${gateway 2} weight ${number}
```

Multipath routes make the system balance packets across several links according to the weight (higher weight is preferred, so gateway/interface with weight of 2 will get roughly two times more traffic than another one with weight of 1). You can have as many gateways as you want and mix gateway and interface routes, like:

```
ip route add default nexthop via 192.168.1.1 weight 1 nexthop dev ppp0 weight 10
```

**Warning:** the downside of this type of balancing is that packets are not guaranteed to be sent back through the same link they came in. This is called "asymmetric routing". For routers that simply forward packets and don't do any local traffic processing such as NAT, this is usually normal, and in some cases even unavoidable.

If your system does anything but forwarding packets between interfaces, this may cause problems with incoming connections and some measures should be taken to prevent it.

可以补充的是如果用的多个等带宽的PPPoE Interface的话，可以只用```nexthop dev ${pppoe-dev}```

### 修改内核运行参数

仅仅是上一步的话，可以见到在使用P2P下载时已经有负载均衡了，但是对HTTP的多线程下载并没有加速效果，因为一些Linux内核对IPv4的ECMP默认设置到L3 Hash，而且也没有开启邻居健康检查，所以需要修改下内核的运行参数，相关的具体的参数说明如下：

> fib_multipath_hash_policy - INTEGER
>	Controls which hash policy to use for multipath routes. Only valid for kernels built with CONFIG_IP_ROUTE_MULTIPATH enabled.
>	Default: 0 (Layer 3)
>	Possible values:
>	0 - Layer 3
>	1 - Layer 4
>	2 - Layer 3 or inner Layer 3 if present

> fib_multipath_use_neigh - BOOLEAN
>	Use status of existing neighbor entry when determining nexthop for multipath routes. If disabled, neighbor information is not used and packets could be directed to a failed nexthop. Only valid for kernels built with CONFIG_IP_ROUTE_MULTIPATH enabled.
>	Default: 0 (disabled)
>	Possible values:
>	0 - disabled
>	1 - enabled

```shell
echo "net.ipv4.fib_multipath_hash_policy=1" >> /etc/sysctl.conf
echo "net.ipv4.fib_multipath_use_neigh=1" >> /etc/sysctl.conf
sysctl -p
```

其他相关的内核运行参数（IPv6 Flowlabel等）可以参考[ip-sysctl.txt](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt)

## OpenWrt上的部署

## 偶然的发现

这里保留当时的发现过程：（当时发现了这个功能非常开心，现在来看部分内容不准确，但是文末的脚本的效果要好于ECMP默认的方法）

只在K2P OpenWrt 18.06上面实践过，对LEDE 17.01无效

下面是在操作一番之后的路由表，对IPv6的测速显示网速翻了四倍

```shell
root@OpenWrt:~# ip -6 route | grep pppoe
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN4 proto static metric 512 pref medium
2000:::/3 dev pppoe-VWAN4 proto static metric 256 pref medium
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-wan weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN4 weight 1
```

大致可以看出这已经是做了负载均衡，需要的只是安装好luci-app-mwan3，简单操作一下路由表，这里的网关地址和dev需要看具体情况

2000::/3 就是IPv6的单播（Unicast）地址了，在不做任何操纵的情况下，无PD的路由表（单拨）

```shell
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 proto static metric 512 pref medium
2001:250:1006:dff0::/64 dev pppoe-VWAN3 proto static metric 256 pref medium
...
```
之后通过下面两条常规的添加路由表条目的命令：
```shell
route -A inet6 add 2000::/3 gw fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2
route -A inet6 add 2000::/3 gw fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3
```
发现路由表就出现了nexthup和负载均衡中的weight标记

```shell
root@OpenWrt:~# ip -6 route | grep pppoe
default from 2001:250:1006:dff0::/64 via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 proto static metric 512 pref medium
2000::/3 dev pppoe-VWAN3 proto static metric 256 pref medium
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN2 weight 1
        nexthop via fe80::96db:daff:fe3e:8fcf dev pppoe-VWAN3 weight 1
...
```

类似的网关添加过程对IPv4效果并不好，这里稍后再细说

之后还需要修改一下防火墙，这里用的还是NAT6中的那条命令

因为对路由表的修改在路由器重启之后会重置，所以还是要添加到开机的启动脚本或者添加到自定的防火墙规则之中，如果已经在NAT6配置过程中配置好了的话这一步就可以忽略了

```shell
ip6tables -t nat -I POSTROUTING -s `uci get network.globals.ula_prefix` -j MASQUERADE
```

实测的峰值速度可以到达双网口的满速，但是一样的问题，在UT，IDM，Thunder这种多线程下载软件下轻松满速，而在YouTube视频应用下，只有半速（甚至包括用IDM下载的情况下）

目前最新版本的mwan3也开始支持IPv6多拨了，但是个人还是觉得日常使用的话用以上的方法更加快捷，不过需要对网络有一定的了解，下面是一段针对IPv6 NAT的多拨hotplug脚本，用了一些OpenWrt开发时推荐的写法，比较实验性

```shell
#!/bin/sh
[ "$ACTION" = ifup ] || exit 0
ifaces=$(ubus call network.interface dump | jsonfilter -e '$.interface[@.proto="dhcpv6" && @.up=true].interface')
for iface_6 in $ifaces
do
  [ "$INTERFACE" = $iface_6 ]  || continue
  devices=$(ubus call network.interface dump | jsonfilter -e '$.interface[@.proto="dhcpv6" && @.up=true].device')
  ipv6_gw=$(ifstatus $iface_6 | jsonfilter -e '$.route[1].nexthop')
  for ipv6_dev in $devices
  do
    status=$(route -A inet6 add 2000::/3 gw $ipv6_gw dev $ipv6_dev 2>&1)
    logger -t NAT6 "Gateway: $ipv6_dev: Done $status"
  done
  exit 0
done
```
